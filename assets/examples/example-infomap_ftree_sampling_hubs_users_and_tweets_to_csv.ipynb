{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample tweets based on community module or hashtag groupings\n",
    "\n",
    "If you are using .ftree files, then follow the below imoprting and process. \n",
    "\n",
    "If you already have edge data, then proceed to importing your tweet corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import .ftree files and process into respective network edge and node data per module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nttc\n",
    "\n",
    "# 1. Retrieve directory of .ftree files and save each line of the file within a list of lists to per Period Dict\n",
    "ftree_path = '../infomap/output/nets/ftree/ftree'\n",
    "\n",
    "# regex is the file pattern in a dedicated directory, e.g., \n",
    "# # r\"\\d{1,2}\" will match the '1' in p1_ftree.ftree\n",
    "dict_map = nttc.batch_map(regex=r\"\\d{1,2}\", path=ftree_path, file_type='ftree')\n",
    "\n",
    "# Print sample ftree modules\n",
    "print(\n",
    "    '1.\\nIndices: ',\n",
    "    dict_map['1']['indices']['ftree_modules'],\n",
    "    '\\n\\nFirst 5 file lines of module section: ',\n",
    "    dict_map['1']['lines'][dict_map['1']['indices']['ftree_modules'][0]:5],\n",
    "    '\\n\\n'\n",
    ")\n",
    "\n",
    "# Print sample ftree links\n",
    "five = dict_map['1']['indices']['ftree_links']['1']['indices'][0]+5\n",
    "print(\n",
    "    '1.\\nIndices for module 1 links: ',\n",
    "    dict_map['1']['indices']['ftree_links']['1']['indices'],\n",
    "    '\\n\\nFirst 5 lines of period 1, module 1 links section: ',\n",
    "    dict_map['1']['lines'][dict_map['1']['indices']['ftree_links']['1']['indices'][0]:five],\n",
    "    '\\n\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check output\n",
    "dict_map['1']['indices']['ftree_links']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_dict_map = dict_map\n",
    "# Process each period's module edge data and stores as a DataFrame.\n",
    "dict_with_edges = nttc.ftree_edge_maker(copy_dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_with_edges['1']['indices']['ftree_links']['1']['df_edges'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take full listified .ftree file and write per Period per Module hubs as a Dict\n",
    "new_dict = dict_with_edges\n",
    "dh = nttc.infomap_hub_maker(new_dict, file_type='ftree', mod_sample_size=10, hub_sample_size=-1)\n",
    "print(\n",
    "    '2.\\nSample hub: ',\n",
    "    dh['1']['info_hub']['1'][:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write edge and node lists per module: \n",
    "## (num of periods, num of modules, Dict of module data from infomap_hub_maker)\n",
    "dict_full = nttc.networks_controller(10,10,dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test outputs\n",
    "dict_full['network']['10']['1']['edges'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_full['network']['10']['1']['nodes'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE SAMPLED OUTPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tweet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned and combined CSV data as pandas dataframe\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '../collection/twint/full-combined'\n",
    "encoded_data_path = '../data/encoded'\n",
    "csv_header = 'id,conversation_id,created_at,date,time,timezone,user_id,username,name,place,tweet,mentions,urls,photos,replies_count,retweets_count,likes_count,location,hashtags,link,retweet,quote_url,video'\n",
    "\n",
    "dtype_dict={\n",
    "    'id': str,\n",
    "    'conversation_id': str,\n",
    "    'username': str,\n",
    "    'user_id': str,\n",
    "    'mentions': str,\n",
    "    'tweet': str,\n",
    "    'hashtags': str,\n",
    "    'link': str,\n",
    "    'user_rt': str,\n",
    "}\n",
    "\n",
    "__encoded_all_file__ = 'cleaned-all-combined.csv'\n",
    "\n",
    "df_all = pd.read_csv(join(encoded_data_path, __encoded_all_file__), \n",
    "                     delimiter=',',\n",
    "                     dtype=dtype_dict)\n",
    "\n",
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"Unnamed\" column\n",
    "del df_all['Unnamed: 0']\n",
    "df_all[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_all[['id', 'date', 'user_id', 'username', 'tweet', 'mentions', 'retweets_count', 'hashtags', 'link']]\n",
    "df_selected[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create desired metadata as per your project: period dates and hashtag groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERIOD DATES\n",
    "ranges = [\n",
    "    ('1', ['2018-01-01', '2018-03-30']),\n",
    "    ('2', ['2018-04-01', '2018-06-12']),\n",
    "    ('3', ['2018-06-13', '2018-07-28']),\n",
    "    ('4', ['2018-07-29', '2018-10-17']),\n",
    "    ('5', ['2018-10-18', '2018-11-24']),\n",
    "    ('6', ['2018-11-25', '2018-12-10']),\n",
    "    ('7', ['2018-12-11', '2018-12-19']),\n",
    "    ('8', ['2018-12-20', '2018-12-25']),\n",
    "    ('9', ['2018-12-26', '2019-02-13']),\n",
    "    ('10', ['2019-02-14', '2019-02-28'])\n",
    "]\n",
    "\n",
    "period_dates = nttc.period_dates_writer(ranges=ranges)\n",
    "\n",
    "# HASHTAG GROUPINGS\n",
    "conservative_hashtag_list = [\n",
    "    '#bordercrisis', '#bordersecurity', '#buildthewall',\n",
    "    '#caravaninvasion', '#illegals',  '#migrantcaravan',\n",
    "    '#nationalemergency', '#ronilsingh'\n",
    "]\n",
    "\n",
    "liberal_keyword_list = [ \n",
    "    {\n",
    "        '#felipegomez': ['felipe alonzo-gomez', 'felipe gomez']\n",
    "    },\n",
    "    {\n",
    "        '#maquin': ['jakelin caal', 'maquín', 'maquin' ]\n",
    "    }\n",
    "]\n",
    "liberal_fbt_list = [\n",
    "    '#familyseparation', '#familiesbelongtogether',\n",
    "    '#felipegomez', '#keepfamiliestogether',\n",
    "    '#maquin', '#noborderwall', '#wherearethechildren',\n",
    "    'jakelin caal', 'maquín', 'maquin', 'felipe alonzo-gomez', \n",
    "    'felipe gomez'\n",
    "]\n",
    "liberal_antishutdown_list = [\n",
    "    '#shutdownstories','#trumpshutdown'\n",
    "]\n",
    "\n",
    "period_dates['1'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample hashtag groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sample 1) certain periods, 2) certain hshtags\n",
    "dict_samples = nttc.infomap_content_sampler(\n",
    "                    dict_full['network'], \n",
    "                    sample_size=50,\n",
    "                    period_dates=period_dates,\n",
    "                    corpus=df_selected,\n",
    "                    sample_type='hashtag_group',\n",
    "                    ht_group=liberal_fbt_list,\n",
    "                    user_threshold=5,\n",
    "                    random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_samples['2']['3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Sample 1) certain periods, 2) certain hshtags\n",
    "dict_samples = nttc.infomap_content_sampler(\n",
    "                    dict_full['network'], \n",
    "                    sample_size=50,\n",
    "                    period_dates=period_dates,\n",
    "                    corpus=df_selected,\n",
    "                    sample_type='modules',\n",
    "                    ht_group=None,\n",
    "                    user_threshold=5,\n",
    "                    random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_samples['10']['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch outputs per module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import csv\n",
    "\n",
    "lister = []\n",
    "for p in dict_samples:\n",
    "    for m in dict_samples[p]:\n",
    "        sub_list = []\n",
    "        print(p,m)\n",
    "        try:\n",
    "            records = dict_samples[p][m]['sample'].to_dict('records')\n",
    "            for r in records:\n",
    "                r['period'] = p\n",
    "                r['module'] = m\n",
    "                lister.append(r)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "lister[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_samples = pd.DataFrame.from_dict(lister)\n",
    "df_full_samples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "cleaned_df_full_samples = df_full_samples.drop_duplicates(subset=['id'], keep='first')\n",
    "print(len(cleaned_df_full_samples), len(df_full_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = cleaned_df_full_samples[['period','module','username','tweet','retweets_count','hashtags','link','mentions','date','id','user_id']]\n",
    "cdf[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.to_csv(join('../infomap/output/nets/ftree/csv', 'ftree_fbt_hashtag_groups_tweet_sample.csv'),\n",
    "                                sep=',',\n",
    "                                encoding='utf-8',\n",
    "                                index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT CSV OF EDGE DATA WITH USERNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OUTPUT EDGES\n",
    "lister_edges = []\n",
    "for p in dict_samples:\n",
    "    for m in dict_samples[p]:\n",
    "        sub_list = []\n",
    "        try:\n",
    "            records = dict_full['network'][p][m]['edges'].to_dict('records')\n",
    "            for r in records:\n",
    "                r['period'] = p\n",
    "                r['module'] = m\n",
    "                lister_edges.append(r)\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "lister_edges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lister_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_edges = pd.DataFrame.from_dict(lister_edges)\n",
    "df_full_edges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_edges.to_csv(join('../infomap/output/nets/ftree/csv', 'infomap_edges_with_names_all_periods.csv'),\n",
    "                                sep=',',\n",
    "                                encoding='utf-8',\n",
    "                                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_samples.to_csv(join('../infomap/output/nets/ftree/csv', 'infomap_tweet_sample_all_periods.csv'),\n",
    "                                sep=',',\n",
    "                                encoding='utf-8',\n",
    "                                index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
